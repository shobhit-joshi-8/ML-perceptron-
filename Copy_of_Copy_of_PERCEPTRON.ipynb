{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shobhit-joshi-8/ML-perceptron-/blob/main/Copy_of_Copy_of_PERCEPTRON.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IC9aOQM8iJC"
      },
      "source": [
        "# Perceptron Learning Algorithm Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZB46BXQ8iJI"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us first understand what np.zeros(2+1) do before implementing Perceptron model"
      ],
      "metadata": {
        "id": "B3RcDFDjKUVt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyuHnNrR8iJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0789c172-f0cd-4871-fedc-537b499cf396"
      },
      "source": [
        "W = np.zeros(2+1)\n",
        "W"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " np.insert(X, 0, 1) is used for horizontal stacking of 1's in 0th column\n"
      ],
      "metadata": {
        "id": "pwwJ1O1EKlBn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szwQn6Yd8iJZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0e6ae53-3164-4fe2-a291-8ba8dbff795d"
      },
      "source": [
        "X=[2,3]\n",
        "np.insert(X, 0, 1)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtpJjmC9N8sp"
      },
      "source": [
        "Understand each code step by step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWdHQsjd8iJh"
      },
      "source": [
        "# initialization code\n",
        "def __init__(self, input_size, lr=5, epochs=2, bias=1):\n",
        "    self.W = np.zeros(input_size+bias)\n",
        "    self.epochs = epochs\n",
        "    self.lr = lr"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fom45Tzh8iJn"
      },
      "source": [
        "# Activation function code which is a simple step function\n",
        "def activation_fn(self, x):\n",
        "        #return (x >= 0).astype(np.float32)\n",
        "        return 1 if x >= 0 else 0"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix_xNDhi8iJs"
      },
      "source": [
        "# Calculating dot product of W and X (input vector) and applying step function\n",
        "def predict(self, x):\n",
        "        z = self.W.T.dot(x)\n",
        "        a = self.activation_fn(z)\n",
        "        return a"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvRpXqxd8iJz"
      },
      "source": [
        "# Perceptron Learning code running all the samples for given epochs or iterations\n",
        "def fit(self, X, OutputLabel):\n",
        "    no_of_smaples=4\n",
        "    for _ in range(self.epochs):\n",
        "        for i in range(no_of_samples):\n",
        "            y = self.predict(X[i])\n",
        "            e = OutputLabel[i] - y\n",
        "            self.W = self.W + self.lr * e * np.insert(X[i], 0, 1)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6zeqwrM8iJ6"
      },
      "source": [
        "# The complete executable code of Perceptron model  in one step "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90IArkfa8iJ8"
      },
      "source": [
        " \n",
        "class Perceptron(object):\n",
        "    \"\"\"Implements a perceptron network\"\"\"\n",
        "    def __init__(self, input_size, lr=5, epochs=100):\n",
        "        self.W = np.zeros(input_size+1)\n",
        "        # add one for bias\n",
        "        self.epochs = epochs\n",
        "        self.lr = lr\n",
        "        self.loss_lst = []\n",
        "    \n",
        "    def activation_fn(self, x):\n",
        "        #return (x >= 0).astype(np.float32)\n",
        "        return 1 if x >= 0 else 0\n",
        " \n",
        "    def predict(self, x):\n",
        "        z = self.W.T.dot(x)\n",
        "        a = self.activation_fn(z)\n",
        "        return a\n",
        " \n",
        "    def fit(self, X, d):\n",
        "        for _ in range(self.epochs):\n",
        "            for i in range(d.shape[0]):\n",
        "                x = np.insert(X[i], 0, 1)\n",
        "                y = self.predict(x)\n",
        "                e = d[i] - y\n",
        "                self.W = self.W + self.lr * e * x\n",
        "            self.loss_lst.append(e)                  "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxkfR3Rr8iKC"
      },
      "source": [
        "# AND GATE EXAMPLE WITH NO OF SAMPLES/RECORDS AS 4 AND EPOCHS AS 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FkER4a68iKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02773415-7962-4f05-cfeb-d854b0ef3962"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    X = np.array([\n",
        "        [0, 0],\n",
        "        [0, 1],\n",
        "        [1, 0],\n",
        "        [1, 1]\n",
        "    ])\n",
        "    d = np.array([0, 1, 1, 0])\n",
        " \n",
        "    perceptron = Perceptron(input_size=2)\n",
        "    perceptron.fit(X, d)\n",
        "    print(perceptron.W)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0. -5.  0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "gckUX4MnQ3ho",
        "outputId": "ec9a95bb-9979-483d-bf47-c0262461ecb4"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "x_axis = [int(x) for x in range(100)]\n",
        "y_axis = perceptron.loss_lst\n",
        "plt.plot(x_axis, y_axis)\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks5fWOcq8iKN"
      },
      "source": [
        "# Using the AND gate data, we should get a weight vector of [-3, 2, 1]. This means that the bias is -3 and the weights are 2 and 1 for x_1 and x_2, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWfEutm18iKP"
      },
      "source": [
        "# LETS TEST MANUALLY\n",
        "# let us test for x=[0, 1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8Zw-a2-8iKR",
        "outputId": "f76c80b6-33f5-40ca-ab30-5149686d25f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x=[1, 0, 1]\n",
        "y= -3*1+2*0+1*1\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-2"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb3JLCbk8iKW"
      },
      "source": [
        "# since it is a negative value on applying activation function we get zero which is correct"
      ]
    }
  ]
}